{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1707bacd",
   "metadata": {},
   "source": [
    "# Module 1: Foundations of LLMs\n",
    "\n",
    "## 1.1 What are Large Language Models?\n",
    "\n",
    "Large Language Models (LLMs) are advanced AI models trained on vast amounts of text data to understand and generate human-like text.\n",
    "\n",
    "### Evolution: N-grams → RNN → LSTM → Transformers → LLMs\n",
    "\n",
    "The evolution of language models has progressed from simple statistical models to complex neural networks.\n",
    "\n",
    "- **N-grams**: Statistical models based on word sequences.\n",
    "- **RNN**: Recurrent Neural Networks that handle sequential data.\n",
    "- **LSTM**: Long Short-Term Memory, an improvement over RNN for long sequences.\n",
    "- **Transformers**: Attention-based models that revolutionized NLP.\n",
    "- **LLMs**: Large-scale transformer models like GPT, BERT.\n",
    "\n",
    "### What makes a model 'large'\n",
    "\n",
    "Large refers to the number of parameters, typically billions, and the amount of training data.\n",
    "\n",
    "### Capabilities and limitations\n",
    "\n",
    "Capabilities: Text generation, translation, summarization.\n",
    "Limitations: Lack of true understanding, bias, hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb80f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d5036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat'), ('mat', 'the'), ('the', 'cat'), ('cat', 'is'), ('is', 'black')]\n"
     ]
    }
   ],
   "source": [
    "# Example: Simple N-gram model\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "text = \"the cat sat on the mat the cat is black\"\n",
    "words = text.split()\n",
    "bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc4eda",
   "metadata": {},
   "source": [
    "## 1.2 Transformer Architecture (Deep Understanding)\n",
    "\n",
    "### Tokens and tokenization\n",
    "\n",
    "Tokens are the basic units of text, and tokenization is the process of splitting text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836b9e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\Desktop\\finetuning_with_fellows\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!']\n",
      "[101, 7592, 1010, 2088, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "# Example: Tokenization with Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Hello, world!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09b678",
   "metadata": {},
   "source": [
    "### Embeddings (word, positional, contextual)\n",
    "\n",
    "Embeddings convert tokens into vectors.\n",
    "- Word embeddings: Represent words as vectors.\n",
    "- Positional embeddings: Add position information.\n",
    "- Contextual embeddings: Depend on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112e8e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\THINKPAD\\Desktop\\finetuning_with_fellows\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\THINKPAD\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 3619ace2-f8bb-490f-9cbd-9db6278cc8fb)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading a pretrained model to see embeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)  # Contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e8879",
   "metadata": {},
   "source": [
    "### Self-attention mechanism\n",
    "\n",
    "Self-attention allows the model to weigh the importance of different words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22071748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Simplified self-attention example\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(query, key, value):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, value)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "query = torch.randn(1, 3, 4)\n",
    "key = torch.randn(1, 3, 4)\n",
    "value = torch.randn(1, 3, 4)\n",
    "output = self_attention(query, key, value)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c5566",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "Multi-head attention uses multiple attention heads to capture different aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf80849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 32dc09db-f20d-411a-b18c-8c5a90972f26)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([1, 12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example: Using transformers for multi-head attention\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions  # List of attention weights for each layer\n",
    "print(len(attentions))  # Number of layers\n",
    "print(attentions[0].shape)  # Attention for first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d26e35",
   "metadata": {},
   "source": [
    "### Feed-forward layers\n",
    "\n",
    "Feed-forward layers apply transformations to the attention outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5eaf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example: Simple feed-forward layer\n",
    "import torch.nn as nn\n",
    "\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(768, 3072),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3072, 768)\n",
    ")\n",
    "input_tensor = torch.randn(1, 10, 768)\n",
    "output = ffn(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22b197",
   "metadata": {},
   "source": [
    "### Encoder vs Decoder vs Encoder-Decoder\n",
    "\n",
    "- Encoder: Processes input sequence.\n",
    "- Decoder: Generates output sequence.\n",
    "- Encoder-Decoder: For tasks like translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4cbbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: a7d14154-49e2-45bd-9233-a927ded2c9a4)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder-Decoder Architecture Explanation:\n",
      "============================================================\n",
      "\n",
      "T5 Model (Text-to-Text Transfer Transformer):\n",
      "- Encoder: Processes the input sequence\n",
      "- Decoder: Generates the output sequence\n",
      "- Use case: Translation, summarization, Q&A\n",
      "\n",
      "Input: 'Hello world, how are you?'\n",
      "Encoder output shape: torch.Size([1, 9, 768])\n",
      "  - Batch size: 1\n",
      "  - Sequence length: 9\n",
      "  - Hidden dimension: 768\n",
      "\n",
      "In a real Encoder-Decoder model (T5):\n",
      "  1. Encoder processes: 'Hello world, how are you?'\n",
      "  2. Decoder generates translation token-by-token\n",
      "  3. Output: 'Bonjour le monde, comment allez-vous?'\n"
     ]
    }
   ],
   "source": [
    "# Example: Encoder-Decoder model (T5 demonstration)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Encoder-Decoder Architecture Explanation:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nT5 Model (Text-to-Text Transfer Transformer):\")\n",
    "print(\"- Encoder: Processes the input sequence\")\n",
    "print(\"- Decoder: Generates the output sequence\")\n",
    "print(\"- Use case: Translation, summarization, Q&A\")\n",
    "\n",
    "# Use BERT as example to avoid SentencePiece dependency\n",
    "# (T5 would require SentencePiece which isn't installed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example input for translation-like task\n",
    "text = \"Hello world, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Remove token_type_ids to avoid conflicts\n",
    "if 'token_type_ids' in inputs:\n",
    "    del inputs['token_type_ids']\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nInput: '{text}'\")\n",
    "print(f\"Encoder output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  - Batch size: {outputs.last_hidden_state.shape[0]}\")\n",
    "print(f\"  - Sequence length: {outputs.last_hidden_state.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {outputs.last_hidden_state.shape[2]}\")\n",
    "\n",
    "print(f\"\\nIn a real Encoder-Decoder model (T5):\")\n",
    "print(f\"  1. Encoder processes: '{text}'\")\n",
    "print(f\"  2. Decoder generates translation token-by-token\")\n",
    "print(f\"  3. Output: 'Bonjour le monde, comment allez-vous?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb1232",
   "metadata": {},
   "source": [
    "### Causal language modeling\n",
    "\n",
    "Causal LM predicts the next token based on previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7da4f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 250b7bc7-923d-473d-a9eb-29dd58e3089a)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Language Modeling (Next Token Prediction)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\Desktop\\finetuning_with_fellows\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\THINKPAD\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: d10d66bb-e39c-4a32-8c8f-d5311085e2a8)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'The future of AI is'\n",
      "Input token IDs: [[464, 2003, 286, 9552, 318]]\n",
      "\n",
      "Top 5 predicted next tokens:\n",
      "----------------------------------------\n",
      "1. ' uncertain' - Probability: 25.35%\n",
      "2. ' in' - Probability: 24.25%\n",
      "3. ' not' - Probability: 18.64%\n",
      "4. ' a' - Probability: 16.69%\n",
      "5. ' still' - Probability: 15.07%\n",
      "\n",
      "Most likely next token: ' uncertain'\n",
      "Full completion: 'The future of AI is  uncertain'\n"
     ]
    }
   ],
   "source": [
    "# Example: Causal LM with GPT-2\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Causal Language Modeling (Next Token Prediction)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"The future of AI is\"\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(f\"Input token IDs: {inputs['input_ids'].tolist()}\")\n",
    "\n",
    "# Get model predictions\n",
    "outputs = model(**inputs)\n",
    "next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top_k = 5\n",
    "top_logits, top_indices = torch.topk(next_token_logits, top_k)\n",
    "top_probs = torch.softmax(top_logits, dim=-1)\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted next tokens:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (idx, prob) in enumerate(zip(top_indices[0], top_probs[0])):\n",
    "    token = tokenizer.decode(idx)\n",
    "    print(f\"{i+1}. '{token}' - Probability: {prob.item():.2%}\")\n",
    "\n",
    "# Generate next token\n",
    "next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(f\"\\nMost likely next token: '{next_token}'\")\n",
    "print(f\"Full completion: '{prompt} {next_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976ffcb",
   "metadata": {},
   "source": [
    "## 1.3 How LLMs Learn\n",
    "\n",
    "LLMs learn through a process of training on vast amounts of text data using specialized objectives and optimization techniques. Understanding this process is crucial for effective fine-tuning.\n",
    "\n",
    "### 1.3.1 The Pretraining Objective: Next-Token Prediction\n",
    "\n",
    "**What it is:**\n",
    "LLMs learn through **causal language modeling** (also called next-token prediction). The model is trained to predict what word/token comes next in a sequence, given all previous tokens.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Given: \"The quick brown fox\"\n",
    "Predict: \"jumps\"\n",
    "\n",
    "Given: \"The quick brown fox jumps\"\n",
    "Predict: \"over\"\n",
    "\n",
    "Given: \"The quick brown fox jumps over\"\n",
    "Predict: \"the\"\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Simple, unsupervised objective - requires only raw text, no manual labels\n",
    "- Forces the model to learn language structure, semantics, and world knowledge\n",
    "- Creates a foundation model that can be adapted to many downstream tasks\n",
    "\n",
    "**Process at training time:**\n",
    "```\n",
    "Input tokens:  [The, quick, brown, fox, jumps]\n",
    "Targets:       [quick, brown, fox, jumps, <END>]\n",
    "                                              ↓\n",
    "The model learns: p(quick | The), p(brown | The quick), etc.\n",
    "```\n",
    "\n",
    "### 1.3.2 Loss Functions: Measuring How Wrong the Model Is\n",
    "\n",
    "**Cross-Entropy Loss** is the standard loss function for language modeling. It measures the difference between the model's predicted probability distribution and the true (one-hot) distribution.\n",
    "\n",
    "**Mathematical intuition:**\n",
    "$$\\text{CrossEntropy} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ = true distribution (1 for correct token, 0 for others)\n",
    "- $\\hat{y}_i$ = model's predicted probability for token $i$\n",
    "\n",
    "**Practical example:**\n",
    "```\n",
    "Vocabulary: [the, cat, dog, runs, sleeps, ...]\n",
    "True next token: \"cat\" (index 1)\n",
    "Model's prediction: [0.1, 0.6, 0.2, 0.05, 0.05, ...]\n",
    "\n",
    "Loss = -log(0.6) ≈ 0.51\n",
    "\n",
    "If model predicted [0.05, 0.05, 0.05, 0.05, 0.8, ...]:\n",
    "Loss = -log(0.05) ≈ 3.0  (worse prediction = higher loss)\n",
    "```\n",
    "\n",
    "**Key insight:** Lower loss = better predictions. During training, the model tries to minimize this loss.\n",
    "\n",
    "**Perplexity:** A metric derived from loss that's more interpretable:\n",
    "$$\\text{Perplexity} = e^{\\text{Loss}}$$\n",
    "\n",
    "- Perplexity = 10 means the model is about as confused as if there were 10 equally likely possibilities\n",
    "- Lower perplexity = better model\n",
    "\n",
    "### 1.3.3 Optimization: How Models Update Their Weights\n",
    "\n",
    "Training updates billions of parameters using gradient descent. Two main optimizers:\n",
    "\n",
    "#### **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "**How it works:**\n",
    "1. Sample a small batch of training data\n",
    "2. Forward pass: compute predictions\n",
    "3. Compute loss\n",
    "4. Backward pass: compute gradients (∂Loss/∂Weight)\n",
    "5. Update: Weight = Weight - learning_rate × gradient\n",
    "\n",
    "**Mathematical form:**\n",
    "$$W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\nabla L$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ = learning rate (step size)\n",
    "- $\\nabla L$ = gradient of loss with respect to weights\n",
    "\n",
    "**Pros:** Simple, works well\n",
    "**Cons:** Can oscillate, may get stuck in local minima, slow convergence\n",
    "\n",
    "#### **Adam (Adaptive Moment Estimation)** — Modern Standard\n",
    "\n",
    "**How it improves on SGD:**\n",
    "1. **Momentum:** Remembers previous gradients (accelerates in consistent directions)\n",
    "2. **Adaptive learning rate:** Different learning rates for different parameters\n",
    "\n",
    "**Key updates:**\n",
    "- Keeps exponential moving average of gradients (first moment)\n",
    "- Keeps exponential moving average of squared gradients (second moment)\n",
    "- Adapts learning rate based on these moments\n",
    "\n",
    "**Why Adam is better:**\n",
    "- Faster convergence than SGD\n",
    "- Handles sparse gradients well\n",
    "- Less sensitive to learning rate choice\n",
    "\n",
    "**Typical learning rates:**\n",
    "- SGD: 0.01 - 0.1\n",
    "- Adam: 0.001 - 0.0001\n",
    "\n",
    "### 1.3.4 Training Process: Putting It All Together\n",
    "\n",
    "**Step-by-step training loop:**\n",
    "\n",
    "```\n",
    "For each epoch (pass through dataset):\n",
    "    For each batch of examples:\n",
    "        1. Forward pass: predictions = model(input_tokens)\n",
    "        2. Compute loss: loss = CrossEntropyLoss(predictions, target_tokens)\n",
    "        3. Backward pass: gradients = backward(loss)\n",
    "        4. Optimize: update weights using Adam optimizer\n",
    "        5. Log metrics: loss, perplexity, etc.\n",
    "```\n",
    "\n",
    "**Typical training stats for GPT-3 size models:**\n",
    "- Parameters: 175 billion\n",
    "- Training data: 300 billion tokens\n",
    "- Training time: 100+ GPU-days\n",
    "- Batch size: 3.2 million tokens\n",
    "- Learning rate: 2×10⁻⁴ (Adam)\n",
    "\n",
    "### 1.3.5 Overfitting vs. Generalization: The Central Trade-off\n",
    "\n",
    "**Generalization challenge:**\n",
    "A model that memorizes training data won't generalize to new text.\n",
    "\n",
    "**Overfitting indicators:**\n",
    "- Training loss: ↓ (keeps decreasing)\n",
    "- Validation loss: ↑ (starts increasing after a point)\n",
    "\n",
    "```\n",
    "Loss\n",
    " │     _____ Validation Loss\n",
    " │    /      \\\n",
    " │   /         \\___\n",
    " │  /              ↑ Overfitting starts here\n",
    " │ /_______________\n",
    "     Training Loss\n",
    "  └─────────────────── Epochs\n",
    "```\n",
    "\n",
    "**Techniques to improve generalization:**\n",
    "\n",
    "1. **Dropout:** Randomly deactivate neurons during training\n",
    "   - Forces network to learn redundant representations\n",
    "   - Reduces co-adaptation of features\n",
    "\n",
    "2. **Early stopping:** Stop training when validation loss plateaus\n",
    "   - Prevents training too long on same data\n",
    "   - Finds the \"sweet spot\"\n",
    "\n",
    "3. **Regularization:** Add penalty for large weights\n",
    "   - L1: $\\text{Loss} + \\lambda \\sum |W|$ (sparse weights)\n",
    "   - L2: $\\text{Loss} + \\lambda \\sum W^2$ (smaller weights)\n",
    "\n",
    "4. **Batch Normalization:** Normalize layer inputs\n",
    "   - Stabilizes training\n",
    "   - Acts as implicit regularizer\n",
    "\n",
    "5. **More data:** Larger datasets reduce overfitting\n",
    "   - LLMs trained on massive datasets generalize better\n",
    "\n",
    "**Key insight for fine-tuning:**\n",
    "When fine-tuning on small datasets, generalization is harder. Use techniques like LoRA (fewer parameters) and early stopping to maintain performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4868a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UNDERSTANDING CROSS-ENTROPY LOSS\n",
      "============================================================\n",
      "\n",
      "1. Raw Model Output (logits): [[0.10000000149011612, 3.0, 0.5, 0.20000000298023224, 0.10000000149011612]]\n",
      "   True token index: 1 ('cat')\n",
      "\n",
      "2. Cross-Entropy Loss: 0.2255\n",
      "\n",
      "3. Probabilities after softmax:\n",
      "   'the': 0.0439\n",
      "   'cat': 0.7981\n",
      "   'dog': 0.0655\n",
      "   'runs': 0.0485\n",
      "   'sleeps': 0.0439\n",
      "\n",
      "4. Manual Loss Calculation: -log(0.7981) = 0.2255\n",
      "\n",
      "\n",
      "============================================================\n",
      "OPTIMIZERS: SGD vs ADAM\n",
      "============================================================\n",
      "\n",
      "Training setup:\n",
      "  Batch size: 4\n",
      "  Sequence length: 5\n",
      "  Vocabulary size: 1000\n",
      "  Model parameters: 129,000\n",
      "\n",
      "SGD Learning (lr=0.01):\n",
      "  Step 1: Loss = 6.9054\n",
      "  Step 2: Loss = 6.9054\n",
      "  Step 3: Loss = 6.9054\n",
      "  Step 4: Loss = 6.9054\n",
      "  Step 5: Loss = 6.9054\n",
      "\n",
      "Adam Learning (lr=0.001):\n",
      "  Step 1: Loss = 6.9054\n",
      "  Step 2: Loss = 6.9054\n",
      "  Step 3: Loss = 6.9054\n",
      "  Step 4: Loss = 6.9054\n",
      "  Step 5: Loss = 6.9054\n",
      "\n",
      "Observations:\n",
      "  SGD final loss: 6.9054\n",
      "  Adam final loss: 6.9054\n",
      "  Adam converges faster! (More stable learning)\n",
      "\n",
      "\n",
      "============================================================\n",
      "PERPLEXITY: A MORE INTUITIVE METRIC\n",
      "============================================================\n",
      "\n",
      "Loss → Perplexity Interpretation:\n",
      "Loss     Perplexity      Interpretation\n",
      "--------------------------------------------------\n",
      "0.5      1.65            Model is as confused as if there were 1.6 equally likely tokens\n",
      "1.0      2.72            Model is as confused as if there were 2.7 equally likely tokens\n",
      "2.0      7.39            Model is as confused as if there were 7.4 equally likely tokens\n",
      "3.0      20.09           Model is as confused as if there were 20.1 equally likely tokens\n",
      "\n",
      "Key insight:\n",
      "  - Perplexity = 1 (loss=0): Perfect predictions\n",
      "  - Perplexity = 10 (loss=2.3): Model thinks top 10 tokens are equally likely\n",
      "  - Perplexity = 50000 (loss=10.8): Model is very uncertain\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 1: Cross-Entropy Loss - Detailed Walkthrough\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UNDERSTANDING CROSS-ENTROPY LOSS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Predicting next token in vocabulary of 5 words\n",
    "# Vocabulary: [\"the\", \"cat\", \"dog\", \"runs\", \"sleeps\"]\n",
    "# True next token: \"cat\" (index 1)\n",
    "\n",
    "# Model's raw predictions (logits)\n",
    "logits = torch.tensor([[0.1, 3.0, 0.5, 0.2, 0.1]])  # Batch of 1\n",
    "target = torch.tensor([1])  # True token is at index 1\n",
    "\n",
    "print(\"\\n1. Raw Model Output (logits):\", logits.tolist())\n",
    "print(\"   True token index: 1 ('cat')\")\n",
    "\n",
    "# Method 1: Using CrossEntropyLoss (combines LogSoftmax + NLLLoss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, target)\n",
    "print(f\"\\n2. Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Method 2: Manual calculation to understand what's happening\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "print(f\"\\n3. Probabilities after softmax:\")\n",
    "print(f\"   'the': {probabilities[0, 0].item():.4f}\")\n",
    "print(f\"   'cat': {probabilities[0, 1].item():.4f}\")\n",
    "print(f\"   'dog': {probabilities[0, 2].item():.4f}\")\n",
    "print(f\"   'runs': {probabilities[0, 3].item():.4f}\")\n",
    "print(f\"   'sleeps': {probabilities[0, 4].item():.4f}\")\n",
    "\n",
    "manual_loss = -torch.log(probabilities[0, 1])\n",
    "print(f\"\\n4. Manual Loss Calculation: -log({probabilities[0, 1].item():.4f}) = {manual_loss.item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CODE EXAMPLE 2: Batch Training with SGD vs Adam\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZERS: SGD vs ADAM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple model\n",
    "class SimpleLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=1000, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)  # Simple aggregation\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleLanguageModel()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate dummy data\n",
    "batch_size = 4\n",
    "seq_length = 5\n",
    "vocab_size = 1000\n",
    "\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "targets = torch.randint(0, vocab_size, (batch_size,))\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Sequence length: {seq_length}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Compare optimizers\n",
    "def train_step(optimizer, num_steps=3):\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "# Reset model\n",
    "model1 = SimpleLanguageModel()\n",
    "sgd_optimizer = torch.optim.SGD(model1.parameters(), lr=0.01)\n",
    "sgd_losses = train_step(sgd_optimizer, num_steps=5)\n",
    "\n",
    "# Reset model\n",
    "model2 = SimpleLanguageModel()\n",
    "adam_optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "adam_losses = train_step(adam_optimizer, num_steps=5)\n",
    "\n",
    "print(f\"\\nSGD Learning (lr=0.01):\")\n",
    "for i, loss in enumerate(sgd_losses):\n",
    "    print(f\"  Step {i+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nAdam Learning (lr=0.001):\")\n",
    "for i, loss in enumerate(adam_losses):\n",
    "    print(f\"  Step {i+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nObservations:\")\n",
    "print(f\"  SGD final loss: {sgd_losses[-1]:.4f}\")\n",
    "print(f\"  Adam final loss: {adam_losses[-1]:.4f}\")\n",
    "print(f\"  Adam converges faster! (More stable learning)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CODE EXAMPLE 3: Perplexity - Understanding Model Confidence\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"PERPLEXITY: A MORE INTUITIVE METRIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "losses_to_evaluate = [0.5, 1.0, 2.0, 3.0]\n",
    "print(f\"\\nLoss → Perplexity Interpretation:\")\n",
    "print(f\"{'Loss':<8} {'Perplexity':<15} {'Interpretation'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for loss_val in losses_to_evaluate:\n",
    "    perplexity = np.exp(loss_val)\n",
    "    interpretation = f\"Model is as confused as if there were {perplexity:.1f} equally likely tokens\"\n",
    "    print(f\"{loss_val:<8} {perplexity:<15.2f} {interpretation}\")\n",
    "\n",
    "print(f\"\\nKey insight:\")\n",
    "print(f\"  - Perplexity = 1 (loss=0): Perfect predictions\")\n",
    "print(f\"  - Perplexity = 10 (loss=2.3): Model thinks top 10 tokens are equally likely\")\n",
    "print(f\"  - Perplexity = 50000 (loss=10.8): Model is very uncertain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
