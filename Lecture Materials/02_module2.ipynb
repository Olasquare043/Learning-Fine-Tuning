{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292e01d1",
   "metadata": {},
   "source": [
    "# Module 2: Fine-tuning Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70ebb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a0e1df9",
   "metadata": {},
   "source": [
    "## 2.1 What is Fine-tuning?\n",
    "\n",
    " - Fine-tuning is the process of adapting a pretrained model to a specific task or domain.\n",
    " - Fine-tuning / training / post-training models customizes its behavior, enhances + injects knowledge, and optimizes performance for domains and specific tasks. \n",
    " \n",
    " - For example:\n",
    "\n",
    "OpenAI’s GPT-5 was post-trained to improve instruction following and helpful chat behavior.\n",
    "\n",
    "The standard method of post training is called **Supervised Fine-Tuning (SFT)**\n",
    "- This is the standard and most common method of post-training.\n",
    "- You give the model examples where the correct answer is already known, and it learns to copy that behavior.\n",
    "\n",
    "----------------------\n",
    "```json\n",
    "User: What is AI?\n",
    "\n",
    "Assistant: AI is the field of building machines that can think and learn.\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "- The model learns, \"When I see a question like this, this is how I should respond.\"\n",
    "\n",
    "\n",
    "\n",
    "**Other methods include;**\n",
    "\n",
    "- Preference Optimization \n",
    "   - Direct Preference Optimization(DPO)\n",
    "   - Odds Ratio Preference Optimization(ORPO)\n",
    "    \n",
    "- Distillation \n",
    "\n",
    "- Reinforcement Learning (RL) \n",
    "   - Group Relative Policy Optimization(GRPO)\n",
    "   - Group Based Supervised Policy Optimization(GSPO)\n",
    "\n",
    "where a model  called an \"agent\" learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties then improves over time.\n",
    "\n",
    "Example: Training a child (Reward good behavior and punishor correct bad behaviour)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff1453",
   "metadata": {},
   "source": [
    "| Method       | Full Meaning                               | Simple Explanation                       |\n",
    "| ------------ | ------------------------------------------ | ---------------------------------------- |\n",
    "| SFT          | Supervised Fine-Tuning                     | Learn from correct examples              |\n",
    "| DPO          | Direct Preference Optimization             | Learn which answer humans prefer         |\n",
    "| ORPO         | Odds Ratio Preference Optimization         | Combines SFT + preference learning       |\n",
    "| Distillation | Knowledge Distillation                     | Big model teaches small model            |\n",
    "| RL           | Reinforcement Learning                     | Learn by reward and penalty              |\n",
    "| GRPO         | Group Relative Policy Optimization         | Compare multiple answers and reward best |\n",
    "| GSPO         | Generalized Supervised Policy Optimization | Structured reward + supervision training |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef5293",
   "metadata": {},
   "source": [
    "##### **Fine-tuning a Pre-trained Model on a Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209bdfdd",
   "metadata": {},
   "source": [
    "By fine-tuning a pre-trained model on a dataset, you can:\n",
    "\n",
    "\n",
    "- **Update + Learn New Knowledge:** Inject and learn new domain-specific information.\n",
    "\n",
    "- **Customize Behavior:** Adjust the model’s tone, personality, or response style.\n",
    "\n",
    "- **Optimize for Tasks:** Improve accuracy and relevance for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d9bff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88bcce46",
   "metadata": {},
   "source": [
    "**Example fine-tuning use-cases:**\n",
    "\n",
    "- Enables LLMs to predict if a headline impacts a company positively or negatively.\n",
    "\n",
    "- Can use historical customer interactions for more accurate and custom responses.\n",
    "\n",
    "- Fine-tune LLM on legal texts for contract analysis, case law research, and compliance.\n",
    "\n",
    "You can think of a fine-tuned model as a specialized agent designed to do specific tasks more effectively and efficiently. Shey you get?\n",
    "\n",
    "\n",
    "Some debate whether to use Retrieval-Augmented Generation (RAG) instead of fine-tuning, but fine-tuning can incorporate knowledge and behaviors directly into the model in ways RAG cannot. In practice, combining both approaches yields the best results - leading to greater accuracy, better usability, and fewer hallucinations.\n",
    "\n",
    "\n",
    "It is important to note that fine-tuning can replicate all of RAG's capabilities, but not vice versa. Does this make any sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59f9d4",
   "metadata": {},
   "source": [
    "##### **Real-World Applications of Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a2e51",
   "metadata": {},
   "source": [
    "Fine-tuning can be applied across various domains and needs. Here are a few practical examples of how it makes a difference:\n",
    "\n",
    "**1. Sentiment Analysis for Finance** – Train an LLM to determine if a news headline impacts a company positively or negatively, tailoring its understanding to financial context.\n",
    "\n",
    "**2. Customer Support Chatbots** – Fine-tune on past customer interactions to provide more accurate and personalized responses in a company’s style and terminology.\n",
    "\n",
    "**3. Legal Document Assistance** – Fine-tune on legal texts (contracts, case law, regulations) for tasks like contract analysis, case law research, or compliance support, ensuring the model uses precise legal language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9a7b4",
   "metadata": {},
   "source": [
    "##### **The Benefits of Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed63e3",
   "metadata": {},
   "source": [
    "Fine-tuning offers several notable benefits beyond what a base model or a purely retrieval-based system can provide\n",
    "\n",
    "1. Fine-Tuning vs. RAG: What’s the Difference?\n",
    "- Fine-tuning can do mostly everything RAG can - but not the other way around. During training, fine-tuning embeds external knowledge directly into the model. This allows the model to handle niche queries, summarize documents, and maintain context without relying on an outside retrieval system. That’s not to say RAG lacks advantages as it is excels at accessing up-to-date information from external databases. It is in fact possible to retrieve fresh data with fine-tuning as well, however it is better to combine RAG with fine-tuning for efficiency.\n",
    "\n",
    "2. Task-Specific Mastery\n",
    "- Fine-tuning deeply integrates domain knowledge into the model. This makes it highly effective at handling structured, repetitive, or nuanced queries, scenarios where RAG-alone systems often struggle. In other words, a fine-tuned model becomes a specialist in the tasks or content it was trained on.\n",
    "\n",
    "3. Independence from Retrieval\n",
    "- A fine-tuned model has no dependency on external data sources at inference time. It remains reliable even if a connected retrieval system fails or is incomplete, because all needed information is already within the model’s own parameters. This self-sufficiency means fewer points of failure in production.\n",
    "\n",
    "4. Faster Responses\n",
    "- Fine-tuned models don’t need to call out to an external knowledge base during generation. Skipping the retrieval step means they can produce answers much more quickly. This speed makes fine-tuned models ideal for time-sensitive applications where every second counts.\n",
    "\n",
    "\n",
    "5. Custom Behavior and Tone\n",
    "- Fine-tuning allows precise control over how the model communicates. This ensures the model’s responses stay consistent with a brand’s voice, adhere to regulatory requirements, or match specific tone preferences. You get a model that not only knows what to say, but how to say it in the desired style.\n",
    "\n",
    "6. Reliable Performance\n",
    "- Even in a hybrid setup that uses both fine-tuning and RAG, the fine-tuned model provides a reliable fallback. If the retrieval component fails to find the right information or returns incorrect data, the model’s built-in knowledge can still generate a useful answer. This guarantees more consistent and robust performance for your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7adf2",
   "metadata": {},
   "source": [
    "#### **Fine-tuning misconceptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35326efa",
   "metadata": {},
   "source": [
    "You may have heard that fine-tuning does not make a model learn new knowledge or RAG performs better than fine-tuning. **That is false**.\n",
    "\n",
    "You can train a specialized coding model with fine-tuning and RL while RAG can’t change the model’s weights and only augments what the model sees at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e76d5e",
   "metadata": {},
   "source": [
    "1. Does Fine-Tuning Add New Knowledge to a Model?\n",
    "\n",
    "**Yes - it absolutely can**. A common myth suggests that fine-tuning doesn’t introduce new knowledge, but in reality it does. If your fine-tuning dataset contains new domain-specific information, the model will learn that content during training and incorporate it into its responses. In effect, fine-tuning can and does teach the model new facts and patterns from scratch.\n",
    "\n",
    "2. Is RAG Always Better Than Fine-Tuning?\n",
    "\n",
    "**Not necessarily**. Many assume RAG will consistently outperform a fine-tuned model, but that’s not the case when fine-tuning is done properly. In fact, a well-tuned model often matches or even surpasses RAG-based systems on specialized tasks. Claims that “RAG is always better” usually stem from fine-tuning attempts that weren’t optimally configured - for example, using insufficient training or  incorrect **LoRA parameters**(<-- abeg keep this word for mind)\n",
    "\n",
    "3. Is Fine-Tuning Expensive?\n",
    "\n",
    "**Not at all!** While full fine-tuning or pretraining can be costly, these are not necessary (pretraining is especially not necessary). In most cases, LoRA or QLoRA fine-tuning can be done for minimal cost. In fact, with Unsloth’s free notebooks for Colab or Kaggle, you can fine-tune models without spending a dime. Better yet, you can even fine-tune locally on your own device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca4033",
   "metadata": {},
   "source": [
    "#### **Pretraining vs Fine-tuning**\n",
    "\n",
    "- **Pretraining:** Training on large, general datasets.\n",
    "- **Fine-tuning:** Training on smaller, task-specific datasets.\n",
    "\n",
    "##### **Why fine-tune instead of prompting**\n",
    "\n",
    "Fine-tuning improves performance for specific tasks, reduces prompt engineering.\n",
    "\n",
    "##### **Domain adaptation vs task adaptation**\n",
    "- **Domain adaptation:** Adapting to a new domain.\n",
    "- **Task adaptation:** Adapting to a new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0236692",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch transformers datasets peft bitsandbytes numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a0c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required packages:\n",
      "----------------------------------------\n",
      "✓ torch: 2.9.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owolabi michael\\Desktop\\Learning-Fine-Tuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ transformers: 4.57.6\n"
     ]
    }
   ],
   "source": [
    "# Verify installations and check versions\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        module = importlib.import_module(package_name)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"✓ {package_name}: {version}\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"✗ {package_name}: NOT FOUND\")\n",
    "        return False\n",
    "\n",
    "print(\"Checking required packages:\")\n",
    "print(\"-\" * 40)\n",
    "check_package(\"torch\")\n",
    "check_package(\"transformers\")\n",
    "check_package(\"datasets\")\n",
    "check_package(\"peft\")\n",
    "print(\"-\" * 40)\n",
    "print(\"If all packages show ✓, you're ready to run the examples below!\")\n",
    "print(\"\\nTo use the models, import them like this:\")\n",
    "print(\"from transformers import AutoModelForSequenceClassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed989ef",
   "metadata": {},
   "source": [
    "## 2.2 Types of Fine-tuning\n",
    "\n",
    "Fine-tuning approaches vary based on computational resources, model size, and specific use cases. Each method has distinct trade-offs between performance, computational cost, and ease of implementation.\n",
    "\n",
    "### 1. Full Fine-tuning\n",
    "\n",
    "**What it is:** Updates all model parameters during training. Every weight and bias in the entire neural network is adjusted.\n",
    "\n",
    "**When to use:**\n",
    "- Unlimited computational resources\n",
    "- Very specialized tasks requiring significant adaptation\n",
    "- Smaller models (< 7B parameters)\n",
    "\n",
    "**Advantages:**\n",
    "- Maximum model adaptation to your specific task\n",
    "- Can achieve the best performance\n",
    "\n",
    "**Disadvantages:**\n",
    "- Extremely computationally expensive\n",
    "- Requires significant memory (24GB+ GPU memory for larger models)\n",
    "- Slow training process\n",
    "- Risk of catastrophic forgetting of general knowledge\n",
    "\n",
    "**Memory requirement:** $\\text{Model Parameters} \\times 4 \\text{ bytes (fp32)} \\times 4 \\text{ (optimizer states)} \\approx$ 16-40GB for 7B models\n",
    "\n",
    "**When NOT to use:**\n",
    "- Limited GPU memory\n",
    "- Large models (>13B parameters)\n",
    "- Production environments with budget constraints\n",
    "\n",
    "### 2. Feature Extraction\n",
    "\n",
    "**What it is:** Freezes all pretrained model weights and uses the model as a fixed feature extractor. Only trains a small head layer on top.\n",
    "\n",
    "**When to use:**\n",
    "- Very limited training data\n",
    "- Task very similar to pretraining task\n",
    "- Minimal computational resources\n",
    "\n",
    "**Advantages:**\n",
    "- Extremely fast and cheap\n",
    "- Preserves all general knowledge\n",
    "- Good for downstream tasks similar to pretraining\n",
    "\n",
    "**Disadvantages:**\n",
    "- Limited adaptation to domain-specific patterns\n",
    "- May underperform on very different tasks\n",
    "- Cannot leverage task-specific representations\n",
    "\n",
    "**Use case:** Using BERT embeddings for semantic similarity on small datasets\n",
    "\n",
    "### 3. Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "The most practical approach for most practitioners. Only trains a small subset of parameters while keeping most weights frozen.\n",
    "\n",
    "#### **3.1 LoRA: Low-Rank Adaptation**\n",
    "\n",
    "**Concept:** Instead of updating the full weight matrix $W$, we learn two smaller matrices $A$ and $B$ such that the weight update is $\\Delta W = A \\times B$ where $A$ and $B$ have much lower rank $r$ than the original matrix.\n",
    "\n",
    "$$W_{\\text{new}} = W_{\\text{original}} + A \\times B$$\n",
    "\n",
    "**Memory reduction:** From 16-40GB to **200-300MB** for 7B models\n",
    "\n",
    "**Trade-offs:**\n",
    "- Rank $r$ (default: 8 or 16) — higher = better performance but more parameters\n",
    "- Alpha parameter — scaling factor for LoRA updates\n",
    "- Target modules — which layers to apply LoRA to (usually attention layers)\n",
    "\n",
    "**When to use:**\n",
    "- Limited GPU memory (most practical scenarios)\n",
    "- Medium to large models (7B-70B parameters)\n",
    "- Production fine-tuning\n",
    "- Multiple fine-tuning tasks (can stack multiple LoRA adapters)\n",
    "\n",
    "#### **3.2 QLoRA: Quantized LoRA**\n",
    "\n",
    "**Concept:** Combines LoRA with 4-bit quantization of the base model. The large model is quantized to 4 bits, and only LoRA adapters are trained in full precision.\n",
    "\n",
    "**Memory reduction:** From 16-40GB to **50-100MB** for 7B models (even more aggressive than LoRA)\n",
    "\n",
    "**Trade-off:** Minimal performance loss while achieving dramatic memory savings\n",
    "\n",
    "**When to use:**\n",
    "- Very limited GPU memory (consumer GPUs like RTX 3090)\n",
    "- Can fine-tune 70B models on 24GB GPU\n",
    "- Best balance of cost and performance\n",
    "\n",
    "#### **3.3 Adapters**\n",
    "\n",
    "**Concept:** Inserts small trainable modules (bottleneck adapters) between layers of the frozen base model.\n",
    "\n",
    "**Advantages:**\n",
    "- Can be more efficient than LoRA for some tasks\n",
    "- Modular approach\n",
    "- Task-specific adaptations\n",
    "\n",
    "#### **3.4 Prefix Tuning**\n",
    "\n",
    "**Concept:** Prepends a learned prefix to the input embeddings without modifying model weights.\n",
    "\n",
    "**Advantages:**\n",
    "- No weight updates needed\n",
    "- Good for generation tasks\n",
    "\n",
    "### 4. Instruction Tuning\n",
    "\n",
    "**What it is:** Fine-tuning on instruction-response pairs to make models follow instructions better.\n",
    "\n",
    "**Format:** `[INSTRUCTION, INPUT]` → `[OUTPUT]`\n",
    "\n",
    "**When to use:**\n",
    "- Want to improve instruction-following capabilities\n",
    "- Building chat or assistant models\n",
    "- Making general models task-specific\n",
    "\n",
    "**Example training data:**\n",
    "```\n",
    "Instruction: Classify the sentiment of this review.\n",
    "Input: \"This movie was absolutely terrible.\"\n",
    "Output: Negative\n",
    "```\n",
    "\n",
    "### 5. Supervised Fine-tuning (SFT)\n",
    "\n",
    "**What it is:** Fine-tuning with labeled input-output pairs. The classic approach where you provide labeled examples of desired behavior.\n",
    "\n",
    "**When to use:**\n",
    "- You have quality labeled data\n",
    "- Clear input-output relationships\n",
    "- Any task with ground truth examples\n",
    "\n",
    "**Example:**\n",
    "- Classification tasks with labeled examples\n",
    "- Named Entity Recognition with annotated text\n",
    "- Question-answering with Q&A pairs\n",
    "\n",
    "**Key consideration:** Quality > Quantity. 1,000 high-quality examples often beat 100,000 noisy examples.\n",
    "\n",
    "### 6. Reinforcement Learning from Human Feedback (RLHF) — Conceptual Overview\n",
    "\n",
    "**What it is:** A two-stage process:\n",
    "1. Train a reward model on human preference judgments\n",
    "2. Use the reward model to fine-tune the language model via reinforcement learning\n",
    "\n",
    "**When to use:**\n",
    "- When you want to align model outputs with human preferences\n",
    "- Quality is hard to define with labels but easy to judge\n",
    "- Building chat models or instruction-following systems\n",
    "\n",
    "**Example:** Human raters compare two model outputs and prefer one. The reward model learns this preference, then RL optimizes the model to maximize predicted reward.\n",
    "\n",
    "**Challenge:** Computationally complex, requires expertise in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf70c03",
   "metadata": {},
   "source": [
    "\n",
    "### Code Examples for Each Fine-tuning Type\n",
    "\n",
    "> **Note:** All required packages (torch, transformers, datasets, peft) are already installed. If you encounter import errors, restart the kernel by clicking \"Restart Kernel\" in the notebook toolbar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel reset and torch reinstalled. Try running the examples again!\n",
      "\n",
      "If the issue persists, click 'Restart Kernel' in the notebook toolbar.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TROUBLESHOOTING: If you see ImportError about PyTorch\n",
    "# ============================================================================\n",
    "# This is a kernel caching issue. Run the cell below to fix it:\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clear the kernel's module cache to reload transformers properly\n",
    "if 'transformers' in sys.modules:\n",
    "    del sys.modules['transformers']\n",
    "if 'torch' in sys.modules:\n",
    "    del sys.modules['torch']\n",
    "\n",
    "# Reinstall torch in the kernel\n",
    "os.system('pip install --upgrade torch --quiet')\n",
    "\n",
    "print(\"Kernel reset and torch reinstalled. Try running the examples again!\")\n",
    "print(\"\\nIf the issue persists, click 'Restart Kernel' in the notebook toolbar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1827a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction Approach\n",
      "==================================================\n",
      "\n",
      "# Strategy: Freeze base model, train only classifier head\n",
      "\n",
      "\n",
      "from transformers import AutoModelForSequenceClassification\n",
      "\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\n",
      "    'bert-base-uncased', num_labels=2\n",
      ")\n",
      "\n",
      "# Freeze ALL base model parameters\n",
      "for param in model.base_model.parameters():\n",
      "    param.requires_grad = False\n",
      "\n",
      "# Only classification head is trainable\n",
      "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
      "print(f\"Trainable parameters: {trainable}\")\n",
      "# Output: ~2,000 parameters (classifier head only)\n",
      "\n",
      "Characteristics:\n",
      "  ✓ Extremely fast training\n",
      "  ✓ Minimal memory requirement\n",
      "  ✓ Preserves general knowledge\n",
      "  ✗ Limited adaptation to domain-specific patterns\n",
      "\n",
      "Best for: Classification with very limited training data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 2: Feature Extraction (Freeze All, Train Only Head)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Feature Extraction Approach\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# Strategy: Freeze base model, train only classifier head\\n\")\n",
    "\n",
    "code_example = \"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=2\n",
    ")\n",
    "\n",
    "# Freeze ALL base model parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only classification head is trainable\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable}\")\n",
    "# Output: ~2,000 parameters (classifier head only)\n",
    "\"\"\"\n",
    "\n",
    "print(code_example)\n",
    "\n",
    "print(\"Characteristics:\")\n",
    "print(\"  ✓ Extremely fast training\")\n",
    "print(\"  ✓ Minimal memory requirement\")\n",
    "print(\"  ✓ Preserves general knowledge\")\n",
    "print(\"  ✗ Limited adaptation to domain-specific patterns\")\n",
    "print(\"\\nBest for: Classification with very limited training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd72c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA (Low-Rank Adaptation) - THE GOLD STANDARD\n",
      "==================================================\n",
      "\n",
      "# LoRA Configuration Example\n",
      "\n",
      "\n",
      "from peft import LoraConfig, TaskType, get_peft_model\n",
      "from transformers import AutoModelForSequenceClassification\n",
      "\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\n",
      "    'bert-base-uncased', num_labels=2\n",
      ")\n",
      "\n",
      "# Define LoRA config\n",
      "lora_config = LoraConfig(\n",
      "    task_type=TaskType.SEQ_CLS,\n",
      "    r=8,                          # Rank (lower = fewer params)\n",
      "    lora_alpha=32,                # Scaling factor\n",
      "    lora_dropout=0.1,\n",
      "    bias=\"none\",\n",
      "    target_modules=[\"query\", \"value\"]  # Attention layers\n",
      ")\n",
      "\n",
      "# Apply LoRA\n",
      "model = get_peft_model(model, lora_config)\n",
      "model.print_trainable_parameters()\n",
      "\n",
      "# Output:\n",
      "# trainable params: 73,728 || all params: 109,482,240\n",
      "# trainable%: 0.067%\n",
      "\n",
      "Key LoRA Parameters:\n",
      "  r (rank):      8-16 typical. Higher = more parameters\n",
      "  lora_alpha:    Usually 2x the rank value\n",
      "  lora_dropout:  0.05-0.1 for regularization\n",
      "  target_modules: Which layers to apply LoRA to\n",
      "\n",
      "Memory: 200-300MB for 7B models (vs 28GB full fine-tuning!)\n",
      "Performance: 95-99% of full fine-tuning quality\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 3: LoRA Fine-tuning (MOST PRACTICAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"LoRA (Low-Rank Adaptation) - THE GOLD STANDARD\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# LoRA Configuration Example\\n\")\n",
    "\n",
    "code_example = \"\"\"\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=2\n",
    ")\n",
    "\n",
    "# Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,                          # Rank (lower = fewer params)\n",
    "    lora_alpha=32,                # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"value\"]  # Attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Output:\n",
    "# trainable params: 73,728 || all params: 109,482,240\n",
    "# trainable%: 0.067%\n",
    "\"\"\"\n",
    "\n",
    "print(code_example)\n",
    "\n",
    "print(\"Key LoRA Parameters:\")\n",
    "print(\"  r (rank):      8-16 typical. Higher = more parameters\")\n",
    "print(\"  lora_alpha:    Usually 2x the rank value\")\n",
    "print(\"  lora_dropout:  0.05-0.1 for regularization\")\n",
    "print(\"  target_modules: Which layers to apply LoRA to\")\n",
    "print(\"\\nMemory: 200-300MB for 7B models (vs 28GB full fine-tuning!)\")\n",
    "print(\"Performance: 95-99% of full fine-tuning quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e62847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLoRA - Extreme Memory Efficiency\n",
      "==================================================\n",
      "\n",
      "# QLoRA = 4-bit Quantization + LoRA\n",
      "\n",
      "\n",
      "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
      "from peft import LoraConfig, get_peft_model, TaskType\n",
      "import torch\n",
      "\n",
      "# 4-bit quantization config\n",
      "bnb_config = BitsAndBytesConfig(\n",
      "    load_in_4bit=True,\n",
      "    bnb_4bit_use_double_quant=True,\n",
      "    bnb_4bit_quant_type=\"nf4\",\n",
      "    bnb_4bit_compute_dtype=torch.bfloat16\n",
      ")\n",
      "\n",
      "# Load 70B model in 4-bit (fits on 24GB GPU!)\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    'meta-llama/Llama-2-70b',\n",
      "    quantization_config=bnb_config,\n",
      "    device_map=\"auto\"\n",
      ")\n",
      "\n",
      "# Add LoRA on top\n",
      "lora_config = LoraConfig(\n",
      "    r=16,\n",
      "    lora_alpha=32,\n",
      "    lora_dropout=0.05,\n",
      "    bias=\"none\",\n",
      "    task_type=TaskType.CAUSAL_LM,\n",
      "    target_modules=[\"q_proj\", \"v_proj\"]\n",
      ")\n",
      "\n",
      "model = get_peft_model(model, lora_config)\n",
      "model.print_trainable_parameters()\n",
      "\n",
      "Memory Requirements:\n",
      "  Full 70B model:    280GB+ (impossible on consumer hardware)\n",
      "  QLoRA approach:    24GB GPU (fits on RTX 4090!)\n",
      "\n",
      "Trade-offs:\n",
      "  ✓ 99.8% memory reduction\n",
      "  ✓ Minimal performance loss (1-2%)\n",
      "  ✓ Can fine-tune models 100x larger locally\n",
      "  ✗ Slightly slower training than LoRA\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 4: QLoRA (Quantized LoRA) - ULTRA MEMORY EFFICIENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"QLoRA - Extreme Memory Efficiency\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# QLoRA = 4-bit Quantization + LoRA\\n\")\n",
    "\n",
    "code_example = \"\"\"\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load 70B model in 4-bit (fits on 24GB GPU!)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-70b',\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Add LoRA on top\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\"\"\"\n",
    "\n",
    "print(code_example)\n",
    "\n",
    "print(\"Memory Requirements:\")\n",
    "print(\"  Full 70B model:    280GB+ (impossible on consumer hardware)\")\n",
    "print(\"  QLoRA approach:    24GB GPU (fits on RTX 4090!)\")\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"  ✓ 99.8% memory reduction\")\n",
    "print(\"  ✓ Minimal performance loss (1-2%)\")\n",
    "print(\"  ✓ Can fine-tune models 100x larger locally\")\n",
    "print(\"  ✗ Slightly slower training than LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction Tuning - Prepare Data\n",
      "==================================================\n",
      "\n",
      "# Data Format: Instruction + Input → Output\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 829.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example formatted training data:\n",
      "--------------------------------------------------\n",
      "Instruction: Classify the sentiment of this review.\n",
      "Input: This product is amazing!\n",
      "Output: Positive\n",
      "--------------------------------------------------\n",
      "\n",
      "Dataset size: 4 examples\n",
      "\n",
      "This format teaches the model to:\n",
      "  1. Read instructions\n",
      "  2. Process inputs\n",
      "  3. Generate correct outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 5: Instruction Tuning - Data Formatting\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Instruction Tuning - Prepare Data\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# Data Format: Instruction + Input → Output\\n\")\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Create instruction tuning dataset\n",
    "instruction_data = {\n",
    "    \"instruction\": [\n",
    "        \"Classify the sentiment of this review.\",\n",
    "        \"Classify the sentiment of this review.\",\n",
    "        \"Translate to French:\",\n",
    "        \"Summarize in 2 sentences:\",\n",
    "    ],\n",
    "    \"input\": [\n",
    "        \"This product is amazing!\",\n",
    "        \"Terrible quality, broke immediately.\",\n",
    "        \"The weather is beautiful today.\",\n",
    "        \"Machine learning transforms industries...\"\n",
    "    ],\n",
    "    \"output\": [\n",
    "        \"Positive\",\n",
    "        \"Negative\",\n",
    "        \"Le temps est magnifique aujourd'hui.\",\n",
    "        \"ML revolutionizes sectors. Impact grows daily.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(instruction_data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Format as prompt-response pairs\n",
    "def format_instruction(examples):\n",
    "    texts = []\n",
    "    for instr, inp, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
    "        text = f\"Instruction: {instr}\\nInput: {inp}\\nOutput: {output}\"\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "formatted = dataset.map(format_instruction, batched=True)\n",
    "\n",
    "print(\"Example formatted training data:\")\n",
    "print(\"-\" * 50)\n",
    "print(formatted[0][\"text\"])\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nDataset size: {len(formatted)} examples\")\n",
    "print(\"\\nThis format teaches the model to:\")\n",
    "print(\"  1. Read instructions\")\n",
    "print(\"  2. Process inputs\")\n",
    "print(\"  3. Generate correct outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Fine-tuning - Complete Example\n",
      "==================================================\n",
      "✓ Created dataset: 4 train, 1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\Desktop\\finetuning_with_fellows\\a_finetune\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\THINKPAD\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 318.49 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 189.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenized data\n",
      "✓ Training configuration set:\n",
      "  - Epochs: 3\n",
      "  - Learning rate: 2e-5\n",
      "  - Batch size: 8\n",
      "\n",
      "# To run training, uncomment and execute:\n",
      "\n",
      "# from transformers import AutoModelForSequenceClassification\n",
      "# model = AutoModelForSequenceClassification.from_pretrained(\n",
      "#     \"bert-base-uncased\", num_labels=2\n",
      "# )\n",
      "# trainer = Trainer(\n",
      "#     model=model,\n",
      "#     args=training_args,\n",
      "#     train_dataset=tokenized['train'],\n",
      "#     eval_dataset=tokenized['test'],\n",
      "# )\n",
      "# trainer.train()\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 6: Supervised Fine-tuning (SFT) - Full Training Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Supervised Fine-tuning - Complete Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create training data\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"This movie is fantastic!\",\n",
    "        \"I loved this product!\",\n",
    "        \"Terrible experience.\",\n",
    "        \"Awful, not recommended.\",\n",
    "        \"Amazing quality and shipping!\",\n",
    "    ],\n",
    "    \"label\": [1, 1, 0, 0, 1]  # 1=positive, 0=negative\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(f\"✓ Created dataset: {len(dataset_split['train'])} train, {len(dataset_split['test'])} test\")\n",
    "\n",
    "# Step 2: Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized = dataset_split.map(tokenize, batched=True)\n",
    "print(f\"✓ Tokenized data\")\n",
    "\n",
    "# Step 3: Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "print(f\"✓ Training configuration set:\")\n",
    "print(f\"  - Epochs: 3\")\n",
    "print(f\"  - Learning rate: 2e-5\")\n",
    "print(f\"  - Batch size: 8\")\n",
    "\n",
    "print(\"\\n# To run training, uncomment and execute:\")\n",
    "print(\"\"\"\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\", num_labels=2\n",
    "# )\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized['train'],\n",
    "#     eval_dataset=tokenized['test'],\n",
    "# )\n",
    "# trainer.train()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e980d8f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary: Which Fine-tuning Method Should YOU Use?\n",
    "\n",
    "**Quick Decision Guide:**\n",
    "\n",
    "| Your Situation | Recommended Method | Why |\n",
    "|---|---|---|\n",
    "| **Very limited data** | Feature Extraction | Fast, cheap, works well on similar tasks |\n",
    "| **Standard scenario** | LoRA | 95-99% quality, 1% cost |\n",
    "| **Limited GPU memory** | QLoRA | Fits 70B models on 24GB |\n",
    "| **Want better following** | Instruction Tuning | Trains on (instruction, output) pairs |\n",
    "| **Have labeled data** | SFT | Standard supervised learning |\n",
    "| **Want best performance** | Full Fine-tuning + RL | Most expensive, but best results |\n",
    "\n",
    "### Comparison: Memory & Speed Trade-offs\n",
    "\n",
    "| Method | Memory | Speed | Performance | Best For |\n",
    "|--------|--------|-------|-------------|----------|\n",
    "| **Full Fine-tuning** | 28GB+ | Slow | Highest (if optimized) | Unlimited budget, small models |\n",
    "| **Feature Extraction** | 2-4GB | Fastest | Lowest | Limited data, simple tasks |\n",
    "| **LoRA** | 0.2-0.3GB | Medium | High (near FT) | **Most practical choice** |\n",
    "| **QLoRA** | 0.05-0.1GB | Medium | Good (slight ↓) | Consumer GPUs, large models |\n",
    "| **Instruction Tuning** | (depends on method) | Medium | High | Chat/assistant models |\n",
    "| **SFT** | (depends on method) | Medium | High | Labeled task data |\n",
    "\n",
    "### Decision Tree: Which Fine-tuning Method to Use?\n",
    "\n",
    "```\n",
    "Do you have quality labeled data?\n",
    "├─ YES → Use Supervised Fine-tuning (SFT)\n",
    "│         └─ Want instruction-following? → Use Instruction Tuning\n",
    "└─ NO → Skip to next question\n",
    "\n",
    "Do you have unlimited GPU memory?\n",
    "├─ YES → Use Full Fine-tuning\n",
    "└─ NO → Use LoRA or QLoRA (RECOMMENDED)\n",
    "        └─ How much GPU memory?\n",
    "           ├─ > 16GB → Use LoRA\n",
    "           └─ < 16GB → Use QLoRA\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280776d1",
   "metadata": {},
   "source": [
    "#### **Choose the Right Model And Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a35030",
   "metadata": {},
   "source": [
    "When preparing for fine-tuning, one of the first decisions you'll face is selecting the right model. Here's a step-by-step guide to help you choose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89f85b",
   "metadata": {},
   "source": [
    "1. Choose a model that aligns with your usecase\n",
    "\n",
    "- E.g. For image-based training, select a vision model such as Llama 3.2 Vision. For code datasets, opt for a specialized model like Qwen Coder 2.5.\n",
    "\n",
    "- Licensing and Requirements: Different models may have specific licensing terms and *system requirements*. Be sure to review these carefully to avoid compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5f302",
   "metadata": {},
   "source": [
    "2. Assess your storage, compute capacity and dataset\n",
    "\n",
    "- Use the unsloth *VRAM guideline to determine the VRAM requirements for the model you’re considering.\n",
    "[guideline](https://unsloth.ai/docs/get-started/fine-tuning-for-beginners/unsloth-requirements#system-requirements)\n",
    "- Your dataset will reflect the type of model you will use and amount of time it will take to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2416a0",
   "metadata": {},
   "source": [
    "3. Select a Model and Parameters\n",
    "\n",
    "- We recommend using the latest model for the best performance and capabilities. For instance, as of January 2025, the leading 70B model is Llama 3.3.\n",
    "\n",
    "- You can stay up to date by exploring our model catalog to find the newest and relevant options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f67c5",
   "metadata": {},
   "source": [
    "4. Choose Between Base and Instruct Models\n",
    "\n",
    "Further details below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b94eae",
   "metadata": {},
   "source": [
    "**Instruct or Base Model?**\n",
    "\n",
    "- When preparing for fine-tuning, one of the first decisions you'll face is whether to use an instruct model or a base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36cb65",
   "metadata": {},
   "source": [
    "**Instruct Models**\n",
    "\n",
    "- Instruct models are pre-trained with built-in instructions, making them ready to use without any fine-tuning. These models, including GGUFs and others commonly available, are optimized for direct usage and respond effectively to prompts right out of the box. Instruct models work with conversational chat templates like ChatML or ShareGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c25f4c",
   "metadata": {},
   "source": [
    "**Base Models**\n",
    "\n",
    "- Base models, on the other hand, are the original pre-trained versions without instruction fine-tuning. These are specifically designed for customization through fine-tuning, allowing you to adapt them to your unique needs. Base models are compatible with instruction-style templates like Alpaca or Vicuna, but they generally do not support conversational chat templates out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47384e",
   "metadata": {},
   "source": [
    "**How To Choose**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3aec32",
   "metadata": {},
   "source": [
    "The decision often depends on the quantity, quality, and type of your data:\n",
    "\n",
    "- 1,000+ Rows of Data: If you have a large dataset with over 1,000 rows, it's generally best to fine-tune the base model.\n",
    "\n",
    "- 300–1,000 Rows of High-Quality Data: With a medium-sized, high-quality dataset, fine-tuning the base or instruct model are both viable options.\n",
    "\n",
    "- Less than 300 Rows: For smaller datasets, the instruct model is typically the better choice. Fine-tuning the instruct model enables it to align with specific needs while preserving its built-in instructional capabilities. This ensures it can follow general instructions without additional input unless you intend to significantly alter its functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
